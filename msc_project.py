# -*- coding: utf-8 -*-
"""MSC_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIPi0D-D4rohtw2-nNRhZAH2ztbGaKjz
"""

# Install Necessary Libraries

!pip install pmdarima
!pip install yfinance
!pip install streamlit
!npm install localtunnel
!pip install plotly

import datetime
import numpy as np
import pandas as pd
from math import sqrt
import yfinance as yf
import pmdarima as pm
import seaborn as sns
import scipy.stats as scs
import statsmodels.api as sm
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, LSTM
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler

import warnings
warnings.filterwarnings("ignore")

"""# ARIMA

### - p (AutoRegressive Order):

Represents the number of lag observations included in the model (the number of autoregressive terms).
In an ARIMA model, the autoregressive part refers to the number of past observations (lags) that are used to predict the current observation.
For example, in an AR(1) model, p=1, meaning the model uses the previous time point to predict the current time point.


### - d (Differencing Order):

Represents the number of times the raw observations are differenced to make the time series stationary.
Differencing is a technique used to remove trends or seasonality in the data. If a time series is not stationary (i.e., its statistical properties change over time), differencing is applied to stabilize the mean of the time series.
For example, if d=1, the model uses the difference between consecutive observations. If d=2, it uses the difference of the differences.

### - q (Moving Average Order):

Represents the number of lagged forecast errors in the prediction equation (the number of moving average terms).
The moving average part refers to the model component that incorporates past forecast errors in the prediction.
For example, in an MA(1) model, q=1, meaning the model uses the previous forecast error to predict the current value.
"""

start = datetime.date(2010, 6, 30)
end = datetime.date(2024, 7, 20)
ticker = "AAPL" # Apple Stock

def check_stationarity(y, wl1=21, wl2=252, lags=40, figsize=(15, 10)):
    """ Checks the stationarity of a pandas Series (default is daily prices or returns),
        using plots, correlograms and the ADF test

        using window length: 21 days, 252 days and 40 lags observation,
    """
    ## Calculating rolling statistics

    rolling_wl1_mean = y.rolling(window=wl1).mean()
    rolling_wl2_mean = y.rolling(window=wl2).mean()
    rolling_wl1_vol = y.rolling(window=wl1).std()
    rolling_wl2_vol = y.rolling(window=wl2).std()

    ## Plotting the price, rolling statistics and correlograms

    fig = plt.figure(figsize=figsize)
    sns.set(font_scale=1)
    layout = (2, 2)
    y_ax = plt.subplot2grid(layout, (0, 0))
    vol_ax = plt.subplot2grid(layout, (0, 1))
    acf_ax = plt.subplot2grid(layout, (1, 0))
    pacf_ax = plt.subplot2grid(layout, (1, 1))

    y.plot(ax=y_ax)
    rolling_wl1_mean.plot(ax=y_ax)
    rolling_wl2_mean.plot(ax=y_ax)

    rolling_wl1_vol.plot(ax=vol_ax)
    rolling_wl2_vol.plot(ax=vol_ax)
    y_ax.set_title('Rolling means over time')
    y_ax.legend(['observed', f'{wl1}-period MA of observed', f'{wl2}-period MA of observed'], loc='best')
    #y_ax.set_ylabel("Gold prices(in INR)/oz.")

    vol_ax.set_title('Rolling volatility over time')
    vol_ax.legend([f'{wl1}-period MA of volatility', f'{wl2}-period MA of volatility'], loc='best')

    sm.graphics.tsa.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.05)
    sm.graphics.tsa.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.05)

    ## Running the Augmented Dickey-Fuller test
    print('--------------------------------------------------------------')
    print('--------- The augmented Dickey-Fuller test results -----------')
    print('--------------------------------------------------------------')
    adftest = adfuller(y, autolag='AIC')
    results = pd.Series(adftest[0:4], index=['Test Statistic','p-value','# of Lags','# of Observations'])
    for key,value in adftest[4].items():
        results[f'Critical Value ({key})'] = '{0:.3f}'.format(value)
    print(results)
    print('--------------------------------------------------------------')

df = yf.download(ticker, start=start, end=end, progress=False)
print(f"Downloaded {df.shape[0]} rows and {df.shape[1]} columns of {ticker} data")

nflx = df.copy()
## Resampling to obtain weekly stock prices with the following rules
## 'Open': first opening price of the month
## 'High': max price of the month
## 'Low': min price of the month
## 'Close' and 'Adj Close': last closing price of the month

nflx = nflx.resample('W').agg({'Open':'first', 'High':'max', 'Low': 'min',
                             'Close':'last', 'Adj Close':'last'})

nflx.drop(columns=["Open", "High", "Low", "Close"], inplace=True)
nflx.rename(columns = {'Adj Close': 'adj_close'}, inplace=True)
print(nflx.tail())

## Checking for null values

nflx[nflx['adj_close'].isnull()]

start_str = (start + pd.Timedelta("5 days")).strftime("%B %Y")
end_str = (end - pd.Timedelta("5 days")).strftime("%B %Y")

start_str

end_str

sns.set(font_scale=1.2)
nflx['adj_close'].plot(figsize=(12, 8), title=f"{ticker} weekly adjusted close prices ({start_str} - {end_str})");

check_stationarity(nflx['adj_close'], wl1=4, wl2=52)

nflx['log_returns'] = np.log(nflx['adj_close']/nflx['adj_close'].shift(1))
nflx.dropna(axis='rows', how='any', inplace=True)

check_stationarity(nflx['log_returns'], wl1=4, wl2=52)

## Defining the model by providing the training set and providing the parameters p, d, q
arima_model = ARIMA(nflx['adj_close'], order=(3, 1, 2))

## Fitting the model, disp=0 is to switch off verbose display
arima_fit1 = arima_model.fit()

## Printing a summary of the model
arima_fit1.summary()

def arima_diagnostics(resids_, figsize=(15, 9), n_lags=40):
    '''
    Diagnoses the fit of an ARIMA model by examining its residuals.
    Returns a chart with multiple plots
    '''
    # Creating placeholder subplots
    M = 2
    N = 2
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(M, N, figsize=figsize)

    r = resids_
    resids_ = (r - np.nanmean(r)) / np.nanstd(r)
    resids_nonmissing = resids_[~(np.isnan(resids_))]

    # Plotting residuals over time
    sns.lineplot(x=np.arange(len(resids_)),
                 y=resids_, ax=ax1)
    ax1.set_title('Standardized residuals')

    # Plotting the distribution of residuals
    x_lim = (-1.96 * 2, 1.96 * 2)
    r_range = np.linspace(x_lim[0], x_lim[1])
    norm_pdf = scs.norm.pdf(r_range)

    sns.distplot(resids_nonmissing, hist=True, kde=True,
                 norm_hist=True, ax=ax2)
    ax2.plot(r_range, norm_pdf, color='green', linewidth=2, label='N(0,1)')
    ax2.set_title('Distribution of standardized residuals')
    ax2.set_xlim(x_lim)
    ax2.legend()

    # Q-Q plot
    qq = sm.qqplot(resids_nonmissing, line='s', ax=ax3)
    ## 's' is for standardized line to compare the plot with a normal distribution
    ax3.set_title('Q-Q plot')

    # ACF plot
    sm.graphics.tsa.plot_acf(resids_, lags=n_lags, ax=ax4, alpha=0.05)
    ax4.set_title('ACF plot')

    return fig

sns.set(font_scale=1.2)
arima_diagnostics(arima_fit1.resid)
plt.tight_layout()
plt.show();

# Running the Ljung-Box test
ljung_box_results = sm.stats.acorr_ljungbox(arima_fit1.resid, lags=[10], return_df=True)

# Plotting the results
fig, ax = plt.subplots(1, figsize=(10, 6))
sns.scatterplot(x=range(len(ljung_box_results['lb_pvalue'])), y=ljung_box_results['lb_pvalue'], ax=ax)
ax.axhline(0.05, ls='--', color='red')
ax.set(title="Ljung-Box test results (after modeling NFLX stock prices)", xlabel='Lags', ylabel='p-value')

plt.show()

## Running the Jarque-Bera test and interpreting its results

from statsmodels.stats.stattools import jarque_bera

jb_test_stat, pvalue, _, _ = jarque_bera(arima_fit1.resid)
print(f"Jarque-Bera statistic: {jb_test_stat:.2f} with p-value: {pvalue:.2f}")

if pvalue < 0.05:
    print("Residuals are likely not normally distributed.")
else:
    print("Residuals are likely normally distributed.")

"""## Automatically finding the best ARIMA fit (using the pmdarima library)"""

nflx = nflx[['adj_close']]

# split our data into train and test set
Ntest = 12
train = nflx.iloc[:-Ntest]
test = nflx.iloc[-Ntest:]

## calling auto arima function and pass seasonal equals false
model = pm.auto_arima(train,
                     error_action='ignore', trace=True,
                     suppress_warnings=True, maxiter=10,
                     seasonal=False)

model = pm.auto_arima(train,
                      start_p=0, max_p=6,
                      start_d=0, max_d=6,
                      start_q=0, max_q=6,
                      error_action='ignore',
                      trace=True,
                      suppress_warnings=True,
                      maxiter=10,
                      seasonal=False)

model.summary()

model.get_params()

def plot_result(model, fulldata, train, test):
    params = model.get_params()
    d = params['order'][1]

    train_pred = model.predict_in_sample(start=d, end=-1)
    test_pred, confint = model.predict(n_periods=Ntest, return_conf_int=True)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(fulldata.index, fulldata, label='data')
    ax.plot(train.index[d:], train_pred, label='fitted')
    ax.plot(test.index, test_pred, label='forecast')
    ax.fill_between(test.index, confint[:,0], confint[:,1], color='red', alpha=0.2)
    ax.legend();

plot_result(model, nflx[['adj_close']], train, test)

def plot_test(model, test):
    test_pred, confint = model.predict(n_periods=Ntest, return_conf_int=True)

    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(test.index, test, label='true')
    ax.plot(test.index, test_pred, label='forecast')
    ax.fill_between(test.index, confint[:,0], confint[:,1], color='red', alpha=0.2)
    ax.legend();

plot_test(model, test)

def rmse(y, t):
    return np.sqrt(np.mean((t - y)**2))

test.reset_index(drop=True, inplace=True)

print("RMSE ARIMA:", rmse(model.predict(Ntest), test.squeeze()))
print("RMSE Naive:", rmse(train.iloc[-1], test))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Assuming 'train' is your training dataset
# Define ranges for p, d, q
p_range = range(0, 7)
d_range = range(0, 7)
q_range = range(0, 7)

# Initialize variables to store the best model and lowest MSE
best_model = None
best_order = None
lowest_mse = float('inf')

# Iterate over all combinations of p, d, q
for p in p_range:
    for d in d_range:
        for q in q_range:
            try:
                # Fit the ARIMA model
                model = ARIMA(train, order=(p, d, q))
                model_fit = model.fit()

                # Predict on training data (for simplicity, here we use the in-sample prediction)
                predictions = model_fit.fittedvalues

                # Calculate Mean Squared Error (MSE)
                mse = mean_squared_error(train[d:], predictions)

                # Print status
                print(f"Order: ({p}, {d}, {q}), MSE: {mse}")

                # Update the best model if current MSE is lower
                if mse < lowest_mse:
                    best_model = model_fit
                    best_order = (p, d, q)
                    lowest_mse = mse
            except Exception as e:
                print(f"Error with order ({p}, {d}, {q}): {e}")

# Print the best model and its details
print(f"Best ARIMA Order: {best_order}")
print(f"Lowest MSE: {lowest_mse}")

# Plotting the best model predictions
plt.figure(figsize=(12, 6))
plt.plot(train, label='Actual')
plt.plot(best_model.fittedvalues, label='Fitted', color='red')
plt.legend()
plt.title(f"Best ARIMA Model: Order {best_order}")
plt.show()

"""# Arima Stocks App"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import yfinance as yf
# import plotly.express as px
# 
# st.title("Stocks App")
# symbol = st.text_input("Enter a stock symbol", "AAPL")
# period = st.text_input("Enter a period", "5d")
# if st.button("Get Quote"):
#     df = yf.Ticker(symbol).history(period)
#     st.write(df)
#     st.plotly_chart(px.line(df, x=df.index, y="Close", title=f'{symbol} Stock Close Price'))

!streamlit run app.py &>/content/logs.txt & curl https://loca.lt/mytunnelpassword

!npx localtunnel --port 8501

"""# LSTM"""

# Set up Yahoo Finance data source
yf.pdr_override()
# Retrieve historical stock data
symbol = "AAPL"
start_date = "2020-01-01"
end_date = "2021-01-01"

# Extract closing prices
closing_prices = df["Close"]
# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(closing_prices.values.reshape(-1, 1))
# Divide data into training and testing sets
train_size = int(len(scaled_data) * 0.8)
train_data, test_data = scaled_data[0:train_size, :], scaled_data[train_size:, :]
test_data_index = closing_prices[train_size:].index

# Function to generate time series dataset for LSTM
def create_dataset(dataset, window_size=1):
    data_x, data_y = [], []
    for i in range(len(dataset) - window_size - 1):
        data_x.append(dataset[i:(i + window_size), 0])
        data_y.append(dataset[i + window_size, 0])
    return np.array(data_x), np.array(data_y)


# Generate time series dataset for LSTM
window_size = 5
train_x, train_y = create_dataset(train_data, window_size)
test_x, test_y = create_dataset(test_data, window_size)

# Reshape input to [samples, time steps, features]
train_x = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))
test_x = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))

# Construct LSTM model
model = Sequential()
model.add(LSTM(50, input_shape=(1, window_size)))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

# Train LSTM model
model.fit(train_x, train_y, epochs=100, batch_size=1, verbose=0)

# Make predictions with LSTM model
train_predict = model.predict(train_x)
test_predict = model.predict(test_x)

# Revert predictions to original scale
train_predict = scaler.inverse_transform(train_predict)
train_y = scaler.inverse_transform([train_y])
test_predict = scaler.inverse_transform(test_predict)
test_y = scaler.inverse_transform([test_y])

# Compute root mean squared error (RMSE) for LSTM
test_rmse = sqrt(mean_squared_error(test_y[0], test_predict[:, 0]))
print("LSTM Test RMSE: ", test_rmse)


# Display actual vs. predicted closing prices
plt.figure(figsize=(12, 6))
plt.plot(closing_prices[:-len(test_data_index)], label="Training data Closing prices")
plt.plot(closing_prices[-len(test_data_index):], label="Actual closing prices")
plt.plot(test_data_index[7:], test_predict[:-1], label="LSTM Predicted closing prices")
plt.xlabel("Date")
plt.ylabel("Closing Price")
plt.title(f"{ticker} Stock Closing Price Prediction using LSTM")
plt.legend()
plt.show()

